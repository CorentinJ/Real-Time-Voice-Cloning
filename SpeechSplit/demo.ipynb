{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo conversion\n",
    "import os \n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from utils import pad_seq_to_2\n",
    "from utils import quantize_f0_numpy\n",
    "from model import Generator_3 as Generator\n",
    "from model import Generator_6 as F0_Converter\n",
    "\n",
    "from utils import pad_seq_to_2, quantize_f0_torch, quantize_f0_numpy\n",
    "from model import InterpLnr\n",
    "\n",
    "import argparse\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from solver import Solver\n",
    "from data_loader import get_loader\n",
    "from hparams import hparams, hparams_debug_string\n",
    "\n",
    "min_len_seq = hparams.min_len_seq\n",
    "max_len_seq = hparams.max_len_seq\n",
    "max_len_pad = hparams.max_len_pad\n",
    "\n",
    "root_dir = 'assets/spmel'\n",
    "feat_dir = 'assets/raptf0'\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = 'cuda:0'\n",
    "Interp = InterpLnr(hparams)\n",
    "\n",
    "G = Generator(hparams).eval().to(device)\n",
    "g_checkpoint = torch.load('assets/265000-G.ckpt', map_location=lambda storage, loc: storage)\n",
    "G.load_state_dict(g_checkpoint['model'])\n",
    "\n",
    "# P = F0_Converter(hparams).eval().to(device)\n",
    "# p_checkpoint = torch.load('assets/640000-P.ckpt', map_location=lambda storage, loc: storage)\n",
    "# P.load_state_dict(p_checkpoint['model'])\n",
    "\n",
    "### Load dataset\n",
    "# For fast training.\n",
    "cudnn.benchmark = True\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "# Data loader.\n",
    "vcc_loader = get_loader(hparams)\n",
    "\n",
    "\n",
    "############################3\n",
    "# Pick First Voice For now (Todo: choose?)\n",
    "\n",
    "metaname = os.path.join(root_dir, \"train.pkl\")\n",
    "meta = pickle.load(open(metaname, \"rb\"))\n",
    "\n",
    "submmeta = meta[1]\n",
    "# Pick first voice\n",
    "speaker_id_name = submmeta[0]\n",
    "emb_org_val = submmeta[1][0]\n",
    "# for speaker_save in sbmt[2]:\n",
    "speaker_save = submmeta[2][0]\n",
    "print(speaker_save[4:])\n",
    "sp_tmp = np.load(os.path.join(root_dir, speaker_save + \".npy\"))\n",
    "f0_tmp = np.load(os.path.join(feat_dir, speaker_save + \".npy\"))\n",
    "\n",
    "x_real_pad = sp_tmp[0:, :]\n",
    "f0_org_val = f0_tmp[0:]\n",
    "len_org_val = np.array([max_len_pad -1])\n",
    "\n",
    "a = x_real_pad[0:len_org_val[0], :]\n",
    "c = f0_org_val[0:len_org_val[0]]\n",
    "\n",
    "a = np.clip(a, 0, 1)\n",
    "\n",
    "# x_real_pad = np.pad(a, ((0,max_len_pad-a.shape[0]),(0,0)), 'constant')\n",
    "# f0_org_val = np.pad(c[:,np.newaxis], ((0,max_len_pad-c.shape[0]),(0,0)), 'constant', constant_values=-1e10)\n",
    "            \n",
    "\n",
    "# data_loader_samp = vcc_loader[2]\n",
    "# data_iter_samp = iter(data_loader_samp)\n",
    "# speaker_id_name, x_real_pad, emb_org_val, f0_org_val, len_org_val = next(data_iter_samp)\n",
    "\n",
    "\n",
    "x_real_pad = torch.from_numpy(np.stack(x_real_pad, axis=0))\n",
    "emb_org_val = torch.from_numpy(np.stack(emb_org_val, axis=0))\n",
    "f0_org_val = torch.from_numpy(np.stack(f0_org_val, axis=0))\n",
    "len_org_val = torch.from_numpy(np.stack(len_org_val, axis=0))\n",
    "\n",
    "x_real_pad =  torch.unsqueeze(x_real_pad.to(device)  , 0)\n",
    "emb_org_val = torch.unsqueeze( emb_org_val.to(device), 0)\n",
    "len_org_val = torch.unsqueeze( len_org_val.to(device), 0)\n",
    "f0_org_val =  torch.unsqueeze(f0_org_val.to(device), 0)\n",
    "\n",
    "# x_real_pad = torch.unsqueeze(x_real_pad, 0)\n",
    "\n",
    "x_f0 = torch.cat((x_real_pad, f0_org_val), dim=-1)\n",
    "x_f0_F = torch.cat((x_real_pad, torch.zeros_like(f0_org_val)), dim=-1)\n",
    "x_f0_C = torch.cat((torch.zeros_like(x_real_pad), f0_org_val), dim=-1)\n",
    "\n",
    "print(x_f0.shape)\n",
    "print(len_org_val.shape)\n",
    "\n",
    "x_f0_intrp = Interp(x_f0, len_org_val) \n",
    "f0_org_intrp = quantize_f0_torch(x_f0_intrp[:,:,-1])[0]\n",
    "x_f0_intrp_org = torch.cat((x_f0_intrp[:,:,:-1], f0_org_intrp), dim=-1)\n",
    "\n",
    "x_f0_F_intrp = Interp(x_f0_F, len_org_val) \n",
    "f0_F_org_intrp = quantize_f0_torch(x_f0_F_intrp[:,:,-1])[0]\n",
    "x_f0_F_intrp_org = torch.cat((x_f0_F_intrp[:,:,:-1], f0_F_org_intrp), dim=-1)\n",
    "\n",
    "x_f0_C_intrp = Interp(x_f0_C, len_org_val) \n",
    "f0_C_org_intrp = quantize_f0_torch(x_f0_C_intrp[:,:,-1])[0]\n",
    "x_f0_C_intrp_org = torch.cat((x_f0_C_intrp[:,:,:-1], f0_C_org_intrp), dim=-1)\n",
    "                        \n",
    "# x_identic_val = G(x_f0_intrp_org, x_real_pad, emb_org_val)\n",
    "# x_identic_woF = G(x_f0_F_intrp_org, x_real_pad, emb_org_val)\n",
    "# x_identic_woR = G(x_f0_intrp_org, torch.zeros_like(x_real_pad), emb_org_val)\n",
    "# x_identic_woC = G(x_f0_C_intrp_org, x_real_pad, emb_org_val)\n",
    "\n",
    "conditions = ['N', 'F', 'R', 'C']\n",
    "spect_vc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for condition in conditions:\n",
    "        if condition == 'N':\n",
    "            x_identic_val = G(x_f0_intrp_org, x_real_pad, emb_org_val)\n",
    "        if condition == 'F':\n",
    "            x_identic_val = G(x_f0_F_intrp_org, x_real_pad, emb_org_val)\n",
    "        if condition == 'R':\n",
    "            x_identic_val = G(x_f0_intrp_org, torch.zeros_like(x_real_pad), emb_org_val)\n",
    "        if condition == 'C':\n",
    "            x_identic_val = G(x_f0_C_intrp_org, x_real_pad, emb_org_val)\n",
    "            \n",
    "        if 'R' in condition:\n",
    "            uttr_trg = x_identic_val[0, :len_org_val[0], :].cpu().numpy()\n",
    "        else:\n",
    "            uttr_trg = x_identic_val[0, :len_org_val[0], :].cpu().numpy()\n",
    "                \n",
    "        spect_vc.append( (speaker_save + \"_\" + condition, uttr_trg ) ) \n",
    "\n",
    "print(len(spect_vc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n",
      "1034-121119-0017_N\n",
      "191\n",
      "100%|██████████| 48896/48896 [05:14<00:00, 155.51it/s]\n",
      "  0%|          | 16/48896 [00:00<05:12, 156.25it/s]1034-121119-0017_F\n",
      "191\n",
      " 25%|██▍       | 12031/48896 [01:19<04:02, 151.84it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ae4bf71d1cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavegen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msoundfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplerate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/BEP/SpeechSplit/synthesis.py\u001b[0m in \u001b[0;36mwavegen\u001b[0;34m(model, c, tqdm)\u001b[0m\n\u001b[1;32m     67\u001b[0m         y_hat = model.incremental_forward(\n\u001b[1;32m     68\u001b[0m             \u001b[0minitial_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             log_scale_min=hparams.log_scale_min)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/speechsplit/lib/python3.6/site-packages/wavenet_vocoder/wavenet.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, initial_input, c, g, T, test_inputs, tqdm, softmax, quantize, log_scale_min)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincremental_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/speechsplit/lib/python3.6/site-packages/wavenet_vocoder/conv.py\u001b[0m in \u001b[0;36mincremental_forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdilation\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/speechsplit/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# spectrogram to waveform\n",
    "import torch\n",
    "import soundfile\n",
    "import pickle\n",
    "import os\n",
    "from synthesis import build_model\n",
    "from synthesis import wavegen\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "model = build_model().to(device)\n",
    "checkpoint = torch.load(\"assets/checkpoint_step001000000_ema.pth\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "print(len(spect_vc))\n",
    "for spect in spect_vc:\n",
    "    name = spect[0]\n",
    "    name = name.split('/')[1]   \n",
    "    print(name)\n",
    "\n",
    "    c = spect[1]\n",
    "    print(len(c))\n",
    "    waveform = wavegen(model, c=c)   \n",
    "    soundfile.write('results/'+name+'.wav', waveform, samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('speechsplit': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1bc36a847e5420bfedfec726f2c07bac0a5c3d5ff08c5aef528395c7f02ba9e9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}